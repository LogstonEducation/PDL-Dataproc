{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"gs://pdl-dataproc/titanic/train.csv\", header=True, inferSchema=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f  # We tend to need this a lot. Let's import it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename label column to make use of PySpark classifier defaults.\n",
    "df = df.withColumnRenamed('Survived', 'label')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's ensure Age has a non-null value. If Age is Null, set it to -1.\n",
    "df = df.withColumn('Age', f.when(f.col(\"Age\").isNull(), -1).otherwise(f.col(\"Age\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's cast the Sex column types to integers so the ML classifier can leverage this data.\n",
    "df = df.withColumn('Sex',\n",
    "    f.when(\n",
    "        f.col('Sex') == 'male', 1\n",
    "    ).when(\n",
    "        f.col('Sex') == 'female', 0\n",
    "    ).otherwise(-1)\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df.select('Embarked').collect())  # Let's see our enum options..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, let's cast the Embarked column to something numeric.\n",
    "df = df.withColumn('Embarked',\n",
    "    f.when(\n",
    "        f.col('Embarked') == 'C', 1\n",
    "    ).when(\n",
    "        f.col('Embarked') == 'Q', 2\n",
    "    ).when(\n",
    "        f.col('Embarked') == 'S', 3\n",
    "    ).otherwise(-1)  \n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar clean up of Fare\n",
    "df = df.withColumn('Fare', f.when(f.col(\"Fare\").isNull(), -1).otherwise(f.col(\"Fare\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up the Cabin column by casting the cabin class to an integer and replacing nulls with -1. \n",
    "df = df.withColumn('Cabin', f.lower(f.col('Cabin')))  # lower case the value\n",
    "df = df.withColumn('Cabin', f.substring(f.col('Cabin'), 0, 1))  # get the first char from the value\n",
    "df = df.withColumn('Cabin', f.ascii(f.col('Cabin')))  # get a number for the value (ASCII code)\n",
    "df = df.withColumn('Cabin', f.when(f.col(\"Cabin\").isNull(), -1).otherwise(f.col(\"Cabin\"))) # repalce NULL -> -1\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of columns to use as features\n",
    "feature_cols = set(df.columns)\n",
    "feature_cols -= {'PassengerId', 'label', 'Name', 'Ticket'}\n",
    "feature_cols = list(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# PySpark likes is features and lables bundled up into VectorAssembler objects.\n",
    "features = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",  # PySpark uses this as the default column\n",
    ").transform(df)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up data for test and training\n",
    "train, test = features.randomSplit([0.7, 0.3], seed=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "model = classifier.fit(train)\n",
    "\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "print('Test Area Under ROC', BinaryClassificationEvaluator().evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"gs://pdl-dataproc/titanic/test.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumnRenamed('Survived', 'label')\n",
    "df = df.withColumn('Age', f.when(f.col(\"Age\").isNull(), -1).otherwise(f.col(\"Age\")))\n",
    "df = df.withColumn('Sex',\n",
    "    f.when(\n",
    "        f.col('Sex') == 'male', 1\n",
    "    ).when(\n",
    "        f.col('Sex') == 'female', 0\n",
    "    ).otherwise(-1)\n",
    ")\n",
    "df = df.withColumn('Embarked',\n",
    "    f.when(\n",
    "        f.col('Embarked') == 'C', 1\n",
    "    ).when(\n",
    "        f.col('Embarked') == 'Q', 2\n",
    "    ).when(\n",
    "        f.col('Embarked') == 'S', 3\n",
    "    ).otherwise(-1)  \n",
    ")\n",
    "df = df.withColumn('Fare', f.when(f.col(\"Fare\").isNull(), -1).otherwise(f.col(\"Fare\")))\n",
    "df = df.withColumn('Cabin', f.lower(f.col('Cabin')))\n",
    "df = df.withColumn('Cabin', f.substring(f.col('Cabin'), 0, 1))\n",
    "df = df.withColumn('Cabin', f.ascii(f.col('Cabin')))\n",
    "df = df.withColumn('Cabin', f.when(f.col(\"Cabin\").isNull(), -1).otherwise(f.col(\"Cabin\")))\n",
    "\n",
    "feature_cols = set(df.columns)\n",
    "feature_cols -= {'PassengerId', 'label', 'Name', 'Ticket'}\n",
    "feature_cols = list(feature_cols)\n",
    "\n",
    "features = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    ").transform(df)\n",
    "\n",
    "predictions = model.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.withColumnRenamed('prediction', 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Return a new RDD that is reduced into numPartitions partitions.\n",
    "rdd = predictions.select(['PassengerId', 'Survived']).coalesce(1)\n",
    "rdd.write.csv('gs://pdl-dataproc/titanic/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# FYI, this could cause OOM errors! All data sent to one node\n",
    "rdd = predictions.select(['PassengerId', 'Survived']).coalesce(1)\n",
    "rdd = rdd.withColumn(\"Survived\", f.col(\"Survived\").cast(IntegerType()))\n",
    "rdd.write.option(\"header\",\"true\").csv('gs://pdl-dataproc/titanic/results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
